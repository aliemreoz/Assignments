{"nbformat": 4, "cells": [{"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfrom pyspark.sql import SparkSession\n\n# @hidden_cell\n# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_8eb7e5fdaa4240cba62b017a11bd6579(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', 'a163084eea8d49a9a8f6bfa978c7c788')\n    hconf.set(prefix + '.username', '6bd8640672ce474a97df18170985958e')\n    hconf.set(prefix + '.password', 'jE--F9s1_1#o*i}3')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', False)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_8eb7e5fdaa4240cba62b017a11bd6579(name)\n\nspark = SparkSession.builder.getOrCreate()\n\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initalized for you.\n# The following variable contains the path to your file on your Object Storage.\npath_1 = \"swift://aaaaa.\" + name + \"/100kReviews.txt\"\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nstopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\",\"i'll\",\"you'll\",\"he'll\",\"she'll\",\"we'll\",\"they'll\",\"i'd\",\"you'd\",\"he'd\",\"she'd\",\"we'd\",\"they'd\",\"i'm\",\"you're\",\"he's\",\"she's\",\"it's\",\"we're\",\"they're\",\"i've\",\"we've\",\"you've\",\"they've\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"don't\",\"doesn't\",\"didn't\",\"won't\",\"wouldn't\",\"shan't\",\"shouldn't\",\"mustn't\",\"can't\",\"couldn't\",\"cannot\",\"could\",\"here's\",\"how's\",\"let's\",\"ought\",\"that's\",\"there's\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\"]\n\n# convert all words to lowercase\n# remove words that are too long or too short \nreviewsRdd = sc.textFile(path_1).map(lambda line: line.lower()).map(lambda line: \" \".join(word for word in line.strip().split() if 25 > len(word) > 4))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nreviewsRdd = reviewsRdd.map(lambda line: \" \".join(word for word in line.strip().split() if word not in stopwords))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfrom pyspark.sql import SQLContext, Row\nrowedReviewsRdd = reviewsRdd.map(lambda (data): Row(data = data))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nreviewsRddDF = sqlContext.createDataFrame(rowedReviewsRdd)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.feature import CountVectorizer, Tokenizer\nfrom pyspark.sql import Row\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ntokenizer = Tokenizer(inputCol=\"data\", outputCol=\"words\")\ncountVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\npipeline = PipelineModel(stages=[tokenizer])\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nmodel = countVectorizer.fit(pipeline.transform(reviewsRddDF))\nresult = model.transform(pipeline.transform(reviewsRddDF))\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nmodelToTrain = result.select(\"words\",\"features\")\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfrom pyspark.ml.clustering import LDA\nmodelToTrain.cache()\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nlda = LDA(k = 2)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\ntrainedModel = lda.fit(modelToTrain)\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nvocabArray = model.vocabulary\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\nfor i in trainedModel.describeTopics(6).rdd.map(tuple).collect():\n    print \"Topic \"+str(i[0])\n    for j in i[1]:\n        print vocabArray[j]\n    print \"\\n\"\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n#The meanings of German result by order\n# Not\n# One\n# Eat\n# Were\n# One\n# something\n\n"], "metadata": {}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["\n \n\n"], "metadata": {}}], "metadata": {}, "nbformat_minor": 1}